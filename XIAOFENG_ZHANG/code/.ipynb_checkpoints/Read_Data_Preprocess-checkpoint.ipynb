{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3fc11b-44e2-42b8-878d-497ab2c6e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4703948-64d5-4562-8b2a-6cd7a49639ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"../data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"../data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"../data/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "    \"../data/Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"../data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "    \"../data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"../data/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"../data/Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0d76e5-364e-46d2-a0b5-fd2d40dabd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Concatenate Datasets\n",
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Combine all data into a single dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599dc7cd-ab33-4d77-afe8-dc09e54fe4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning\n",
    "# Drop any columns with all NaN values and drop rows with any NaN values\n",
    "cleaned_df = combined_df.dropna(axis=1, how='all')  # Drop columns with all NaN\n",
    "cleaned_df = cleaned_df.dropna()  # Drop rows with any NaN values\n",
    "\n",
    "# Remove leading and trailing whitespaces from column names\n",
    "cleaned_df.columns = cleaned_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9e0adb-df23-4f53-ba96-269f95c3f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Selection\n",
    "# Separate features and labels\n",
    "features = cleaned_df.drop(columns=['Label'])\n",
    "labels = cleaned_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec2d165-0db0-450d-bfca-dd28ffcc08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle Infinite Values\n",
    "# Replace infinite values with NaN and then drop rows containing NaN values\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "# Update labels to match the cleaned features\n",
    "labels = labels[features.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6426b1c7-5628-4980-ad2e-fc82dbf20103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Label Encoding\n",
    "# Encode categorical labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d53ede-3635-40a5-a37f-1f1ab7b7d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Standardization\n",
    "# Standardize the feature columns to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3c17e0-5a32-4ed8-9547-377cf338bd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1696725, 78)\n",
      "Validation set size: (565575, 78)\n",
      "Test set size: (565576, 78)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train, Validation, Test Split\n",
    "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(scaled_features, encoded_labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the size of each dataset\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f64a28f-0d7b-4d5c-a9e7-a6e3f4161918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]), (0, 14))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the unique values and their range for the encoded labels\n",
    "unique_labels = np.unique(encoded_labels)\n",
    "label_range = (unique_labels.min(), unique_labels.max())\n",
    "\n",
    "(unique_labels, label_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccbe50-35d5-4019-bc02-d35c921c5155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253b5e44-4f62-45c6-b050-b64b274a0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae567ba7-5184-4dcf-95a4-e74ea67d1b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3b077a-6ec8-4adf-b065-d382ea752920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalLearningModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(IncrementalLearningModel, self).__init__()\n",
    "        # Improved feedforward neural network with additional hidden layers and more neurons\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        # Check for NaN or infinite values in output\n",
    "        assert torch.isfinite(x).all(), \"Model output contains NaN or infinite values\"\n",
    "        return x\n",
    "\n",
    "    def incremental_train(self, train_data, train_labels, val_data, val_labels, num_epochs=50, lr=1e-4):\n",
    "        # Ensure labels are of type LongTensor\n",
    "        train_labels = train_labels.long()\n",
    "        val_labels = val_labels.long()\n",
    "        \n",
    "        # Set model to training mode\n",
    "        self.train()\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        class_weights = torch.ones(self.fc5.out_features).to(device)  # Assuming balanced classes initially\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)  # Changed back to Adam for faster convergence\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Learning rate scheduler\n",
    "        \n",
    "        # Create DataLoader for training and validation data\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Check for NaN or infinite values in loss\n",
    "                assert torch.isfinite(loss).all(), \"Loss contains NaN or infinite values\"\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip gradients to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Check gradients for NaN or infinite values\n",
    "                for name, param in self.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        assert torch.isfinite(param.grad).all(), f\"Gradient for {name} contains NaN or infinite values\"\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Step the learning rate scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Validation step\n",
    "            val_loss = 0.0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = self(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    val_loss += loss.item()\n",
    "            self.train()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, num_epochs=50, lr=1e-4, poisoning_rate=0.0, poisoning_strategy='label_flip'):\n",
    "        # Convert numpy arrays to torch tensors\n",
    "        train_data = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        train_labels = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        val_data = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "        val_labels = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Apply data poisoning if specified\n",
    "        if poisoning_rate > 0:\n",
    "            train_data, train_labels = poison_data(train_data, train_labels, poisoning_rate, poisoning_strategy)\n",
    "        \n",
    "        # Train the model\n",
    "        self.incremental_train(train_data, train_labels, val_data, val_labels, num_epochs, lr)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "            outputs = self(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        return predicted\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        # Convert numpy arrays to torch tensors\n",
    "        test_data = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        test_labels = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Predict the labels for the test data\n",
    "        predictions = self.predict(test_data)\n",
    "        # Calculate accuracy and F1 score\n",
    "        accuracy = accuracy_score(test_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        f1 = f1_score(test_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n",
    "        print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21034290-dfa7-4439-a0f9-c0a0fb2556dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle catastrophic forgetting using Replay mechanism\n",
    "def replay_mechanism(model, previous_data, new_data, previous_labels, new_labels, num_epochs=5, lr=1e-4):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    previous_data = torch.tensor(previous_data, dtype=torch.float32).to(device)\n",
    "    new_data = torch.tensor(new_data, dtype=torch.float32).to(device)\n",
    "    previous_labels = torch.tensor(previous_labels, dtype=torch.long).to(device)\n",
    "    new_labels = torch.tensor(new_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Ensure label values are in the correct range\n",
    "    assert previous_labels.min() >= 0 and previous_labels.max() < model.fc5.out_features, f\"Previous labels are out of range: min={previous_labels.min()}, max={previous_labels.max()}\"\n",
    "    assert new_labels.min() >= 0 and new_labels.max() < model.fc5.out_features, f\"New labels are out of range: min={new_labels.min()}, max={new_labels.max()}\"\n",
    "    \n",
    "    # Check for NaN or infinite values in data\n",
    "    assert torch.isfinite(previous_data).all(), \"Previous data contains NaN or infinite values\"\n",
    "    assert torch.isfinite(new_data).all(), \"New data contains NaN or infinite values\"\n",
    "    \n",
    "    # Combine previous and new data\n",
    "    combined_data = torch.cat((previous_data, new_data), dim=0)\n",
    "    combined_labels = torch.cat((previous_labels, new_labels), dim=0)\n",
    "\n",
    "    # Retrain the model with combined dataset\n",
    "    model.incremental_train(combined_data, combined_labels, combined_data, combined_labels, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c450c42-bf64-42b8-bac0-0fd06454f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate poisoned data\n",
    "def poison_data(data, labels, poisoning_rate=0.1, strategy='label_flip'):\n",
    "    \"\"\"\n",
    "    Generate poisoned data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array or torch.Tensor, original input data\n",
    "    - labels: np.array or torch.Tensor, original labels\n",
    "    - poisoning_rate: float, proportion of data to be poisoned\n",
    "    - strategy: str, the type of poisoning strategy ('label_flip' or 'feature_perturbation')\n",
    "\n",
    "    Returns:\n",
    "    - poisoned_data: torch.Tensor, data with poisoning applied\n",
    "    - poisoned_labels: torch.Tensor, labels with poisoning applied\n",
    "    \"\"\"\n",
    "    data = data.clone() if isinstance(data, torch.Tensor) else torch.tensor(data, dtype=torch.float32)\n",
    "    labels = labels.clone() if isinstance(labels, torch.Tensor) else torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    num_samples = data.shape[0]\n",
    "    num_poisoned = int(poisoning_rate * num_samples)\n",
    "    poisoned_indices = random.sample(range(num_samples), num_poisoned)\n",
    "\n",
    "    if strategy == 'label_flip':\n",
    "        poisoned_labels = labels.clone()\n",
    "        # Flip labels to the next class (cyclically)\n",
    "        for idx in poisoned_indices:\n",
    "            poisoned_labels[idx] = (labels[idx] + 1) % len(torch.unique(labels))\n",
    "        return data, poisoned_labels\n",
    "\n",
    "    elif strategy == 'feature_perturbation':\n",
    "        poisoned_data = data.clone()\n",
    "        # Add small perturbations to features of the poisoned data\n",
    "        perturbation = torch.randn_like(poisoned_data[poisoned_indices]) * 0.1\n",
    "        poisoned_data[poisoned_indices] += perturbation\n",
    "        return poisoned_data, labels\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported poisoning strategy. Use 'label_flip' or 'feature_perturbation'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42019b65-5b29-49d2-8376-a347bebfddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment to evaluate model under different poisoning levels and use replay mechanism\n",
    "def run_poisoning_experiment_with_replay(model, X_train, y_train, X_val, y_val, X_test, y_test, poisoning_rates, replay_data, replay_labels, num_epochs=50, lr=1e-4):\n",
    "    results = []\n",
    "\n",
    "    for poisoning_rate in poisoning_rates:\n",
    "        print(f\"\\nRunning experiment with poisoning rate: {poisoning_rate:.2f}\")\n",
    "\n",
    "        # Initialize a new model for each experiment\n",
    "        model_instance = IncrementalLearningModel(input_size=X_train.shape[1], num_classes=len(np.unique(y_train))).to(device)\n",
    "\n",
    "        # Poison data\n",
    "        poisoned_data, poisoned_labels = poison_data(X_train, y_train, poisoning_rate=poisoning_rate, strategy='label_flip')\n",
    "\n",
    "        # Train model\n",
    "        model_instance.train_model(poisoned_data, poisoned_labels, X_val, y_val, num_epochs=num_epochs, lr=lr)\n",
    "\n",
    "        # Evaluate model before replay mechanism\n",
    "        print(\"\\nEvaluating model before replay...\")\n",
    "        accuracy_before, f1_before = model_instance.evaluate(X_test, y_test)\n",
    "\n",
    "        # Apply replay mechanism\n",
    "        print(\"\\nApplying replay mechanism...\")\n",
    "        replay_mechanism(model_instance, replay_data, poisoned_data[:50], replay_labels, poisoned_labels[:50], num_epochs=5, lr=lr)\n",
    "\n",
    "        # Evaluate model after replay mechanism\n",
    "        print(\"\\nEvaluating model after replay...\")\n",
    "        accuracy_after, f1_after = model_instance.evaluate(X_test, y_test)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"poisoning_rate\": poisoning_rate,\n",
    "            \"accuracy_before\": accuracy_before,\n",
    "            \"f1_score_before\": f1_before,\n",
    "            \"accuracy_after\": accuracy_after,\n",
    "            \"f1_score_after\": f1_after\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96af689-8e83-4773-adfd-1aff4d369388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with poisoning rate: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxf19\\AppData\\Local\\Temp\\ipykernel_35912\\75448235.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
      "C:\\Users\\zxf19\\AppData\\Local\\Temp\\ipykernel_35912\\75448235.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(y_train, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.0737, Val Loss: 0.0573, Learning Rate: 0.000100\n",
      "Epoch [2/50], Train Loss: 0.0504, Val Loss: 0.0507, Learning Rate: 0.000100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "    num_classes = len(np.unique(y_train))  # Number of classes\n",
    "\n",
    "    model = IncrementalLearningModel(input_size, num_classes).to(device)\n",
    "\n",
    "    # Run poisoning experiments with replay mechanism\n",
    "    poisoning_rates = [0.0, 0.1, 0.2, 0.3]  # Different levels of poisoning rates to evaluate\n",
    "    replay_data = X_train[:50]\n",
    "    replay_labels = y_train[:50]\n",
    "\n",
    "    experiment_results = run_poisoning_experiment_with_replay(model, X_train, y_train, X_val, y_val, X_test, y_test, poisoning_rates, replay_data, replay_labels)\n",
    "\n",
    "    # Print results\n",
    "    for result in experiment_results:\n",
    "        print(f\"Poisoning Rate: {result['poisoning_rate']:.2f}, \"\n",
    "              f\"Accuracy Before Replay: {result['accuracy_before']:.4f}, F1 Score Before Replay: {result['f1_score_before']:.4f}, \"\n",
    "              f\"Accuracy After Replay: {result['accuracy_after']:.4f}, F1 Score After Replay: {result['f1_score_after']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb576b7b-58b1-4280-9316-84deb266f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33447775-b6f9-4424-a3bf-e88e61f114d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the impact of poisoned data on model performance\n",
    "def visualize_poisoning_impact(results):\n",
    "    poisoning_rates = [result[\"poisoning_rate\"] for result in results]\n",
    "    accuracies = [result[\"accuracy\"] for result in results]\n",
    "    f1_scores = [result[\"f1_score\"] for result in results]\n",
    "\n",
    "    # Plot Accuracy vs Poisoning Rate\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(poisoning_rates, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "    plt.xlabel('Poisoning Rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Poisoning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(poisoning_rates)\n",
    "    plt.legend()\n",
    "\n",
    "    # F1 Score plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(poisoning_rates, f1_scores, marker='o', linestyle='-', color='r', label='F1 Score')\n",
    "    plt.xlabel('Poisoning Rate')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Poisoning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(poisoning_rates)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_results = [\n",
    "        {\"poisoning_rate\": 0.0, \"accuracy\": 0.85, \"f1_score\": 0.83},\n",
    "        {\"poisoning_rate\": 0.1, \"accuracy\": 0.75, \"f1_score\": 0.73},\n",
    "        {\"poisoning_rate\": 0.2, \"accuracy\": 0.65, \"f1_score\": 0.62},\n",
    "        {\"poisoning_rate\": 0.3, \"accuracy\": 0.55, \"f1_score\": 0.52},\n",
    "    ]\n",
    "\n",
    "    # Visualize the impact of poisoning\n",
    "    visualize_poisoning_impact(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75231ce2-201b-41ea-8ec5-e68789e103f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_replay_impact(results):\n",
    "    poisoning_rates = [result[\"poisoning_rate\"] for result in results]\n",
    "    accuracy_before = [result[\"accuracy_before\"] for result in results]\n",
    "    accuracy_after = [result[\"accuracy_after\"] for result in results]\n",
    "    f1_before = [result[\"f1_score_before\"] for result in results]\n",
    "    f1_after = [result[\"f1_score_after\"] for result in results]\n",
    "\n",
    "    # Plot Accuracy before and after Replay\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy bar plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    width = 0.3\n",
    "    x = np.arange(len(poisoning_rates))\n",
    "    plt.bar(x - width/2, accuracy_before, width, label='Before Replay', color='b')\n",
    "    plt.bar(x + width/2, accuracy_after, width, label='After Replay', color='g')\n",
    "    plt.xlabel('Poisoning Rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Before and After Replay')\n",
    "    plt.xticks(x, [f\"{rate:.2f}\" for rate in poisoning_rates])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    # F1 Score bar plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(x - width/2, f1_before, width, label='Before Replay', color='r')\n",
    "    plt.bar(x + width/2, f1_after, width, label='After Replay', color='g')\n",
    "    plt.xlabel('Poisoning Rate')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score Before and After Replay')\n",
    "    plt.xticks(x, [f\"{rate:.2f}\" for rate in poisoning_rates])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_results_with_replay = [\n",
    "        {\"poisoning_rate\": 0.0, \"accuracy_before\": 0.85, \"f1_score_before\": 0.83, \"accuracy_after\": 0.85, \"f1_score_after\": 0.83},\n",
    "        {\"poisoning_rate\": 0.1, \"accuracy_before\": 0.75, \"f1_score_before\": 0.73, \"accuracy_after\": 0.78, \"f1_score_after\": 0.76},\n",
    "        {\"poisoning_rate\": 0.2, \"accuracy_before\": 0.65, \"f1_score_before\": 0.62, \"accuracy_after\": 0.70, \"f1_score_after\": 0.68},\n",
    "        {\"poisoning_rate\": 0.3, \"accuracy_before\": 0.55, \"f1_score_before\": 0.52, \"accuracy_after\": 0.60, \"f1_score_after\": 0.58},\n",
    "    ]\n",
    "\n",
    "    # Visualize the impact of replay mechanism\n",
    "    visualize_replay_impact(experiment_results_with_replay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch_gpu",
   "language": "python",
   "name": "tf_pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
